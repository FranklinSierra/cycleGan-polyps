{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\";\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"5\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='red'>**CycleGan embedding classification**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import csv\n",
    "import imageio\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "import pandas as pd\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Input, Conv2D, MaxPooling2D, Dropout, Flatten\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import SGD, Adam, RMSprop\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Model\n",
    "from joblib import dump, load"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data analysis\n",
    "### Gen A embeddings\n",
    "**Nota:** Gen A takes white light images and translate it into Nbi representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Downsamplig strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Loading original data embeddings (all are sorted).\n",
    "2. Get the lowest amount of frames that belongs a video\n",
    "3. From the original embedding representation make downsampling approach\n",
    "4. Make a stratified 4 fold-cross validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== data info ====\n",
      "ade dim: (23319, 4096), amount of labels: (23319,), videos: (23319,)\n",
      "hyp dim: (8218, 4096), amount of labels: (8218,), videos: (8218,)\n",
      "ser dim: (7227, 4096), amount of labels: (7227,), videos: (7227,)\n"
     ]
    }
   ],
   "source": [
    "#downsampling es para cuando se quiera balancear los datos por el minimo de muestras y \n",
    "#videos disponibles para entrenar (ej. la clase serrated tiene 12 videos para testear con cada video con 164 \n",
    "#frames entonces downsampling==True quiere decir que para todos los videos se van a tomar 12 con 164 frames c/u)\n",
    "downsampling = True\n",
    "\n",
    "generator = 'GenB'\n",
    "path = \"../data/embeddings/\" + generator + \"/correct_inputs/\"\n",
    "ade_dat = np.load(path + \"AdenomaEmbeddings.npy\")\n",
    "ade_lab = np.load(path + \"AdenomaLabels.npy\")\n",
    "ade_vid = np.load(path + \"AdenomaVideos.npy\") \n",
    "\n",
    "hyp_dat = np.load(path + \"HiperplasticEmbeddings.npy\")\n",
    "hyp_lab = np.load(path + \"HiperplasticLabels.npy\")\n",
    "hyp_vid = np.load(path + \"HiperplasticVideos.npy\")\n",
    "\n",
    "ser_dat = np.load(path + \"SerratedEmbeddings.npy\")\n",
    "ser_lab = np.load(path + \"SerratedLabels.npy\")\n",
    "ser_vid = np.load(path + \"SerratedVideos.npy\")\n",
    "\n",
    "print(\"==== data info ====\")\n",
    "print(\"ade dim: {}, amount of labels: {}, videos: {}\".format(ade_dat.shape, ade_lab.shape, ade_vid.shape))\n",
    "print(\"hyp dim: {}, amount of labels: {}, videos: {}\".format(hyp_dat.shape, hyp_lab.shape, hyp_vid.shape))\n",
    "print(\"ser dim: {}, amount of labels: {}, videos: {}\".format(ser_dat.shape, ser_lab.shape, ser_vid.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **1.1 Data filtering for cycleGan train/test videos**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = np.concatenate((ade_dat, hyp_dat, ser_dat), axis=0)\n",
    "labels = np.concatenate((ade_lab, hyp_lab, ser_lab), axis=0)\n",
    "videos = np.concatenate((ade_vid, hyp_vid, ser_vid), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76\n"
     ]
    }
   ],
   "source": [
    "(unique, counts) = np.unique(videos, return_counts=True)\n",
    "freq = np.asarray((unique, counts)).T\n",
    "print(len(freq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>features</th>\n",
       "      <th>label</th>\n",
       "      <th>video</th>\n",
       "      <th>info</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>38759</th>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>serrated</td>\n",
       "      <td>video_76</td>\n",
       "      <td>serrated/video_76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38760</th>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>serrated</td>\n",
       "      <td>video_76</td>\n",
       "      <td>serrated/video_76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38761</th>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>serrated</td>\n",
       "      <td>video_76</td>\n",
       "      <td>serrated/video_76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38762</th>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>serrated</td>\n",
       "      <td>video_76</td>\n",
       "      <td>serrated/video_76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38763</th>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>serrated</td>\n",
       "      <td>video_76</td>\n",
       "      <td>serrated/video_76</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                features     label     video  \\\n",
       "38759  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  serrated  video_76   \n",
       "38760  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  serrated  video_76   \n",
       "38761  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  serrated  video_76   \n",
       "38762  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  serrated  video_76   \n",
       "38763  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  serrated  video_76   \n",
       "\n",
       "                    info  \n",
       "38759  serrated/video_76  \n",
       "38760  serrated/video_76  \n",
       "38761  serrated/video_76  \n",
       "38762  serrated/video_76  \n",
       "38763  serrated/video_76  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame({'features': list(features), 'label': labels, 'video': videos}, columns=['features', 'label', 'video'])\n",
    "df['info'] = df['label'] + '/' + df['video']\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hiperplastic/video_53', 'serrated/video_70', 'hiperplastic/video_51', 'adenoma/video_21', 'adenoma/video_33', 'adenoma/video_9', 'adenoma/video_10', 'adenoma/video_36', 'adenoma/video_22', 'hiperplastic/video_43', 'adenoma/video_19', 'adenoma/video_14', 'hiperplastic/video_41', 'serrated/video_66', 'serrated/video_71'}\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "root = '../../../data/polyps/testA/'\n",
    "test = []\n",
    "files = os.listdir(root)\n",
    "for file in files:\n",
    "    info = file.split('.')[0]\n",
    "    video_number = info.split('_')[-3]\n",
    "    clase = info.split('_')[0]\n",
    "    if clase == 'adenoma':\n",
    "        name = clase + '/video_' + video_number\n",
    "        test.append(name)\n",
    "    elif clase == 'hiperplastic':\n",
    "        video_number = int(video_number) + 40\n",
    "        name = clase + '/video_' + str(video_number)\n",
    "        test.append(name)\n",
    "    else:\n",
    "        video_number = int(video_number) + 61\n",
    "        name = clase + '/video_' + str(video_number)\n",
    "        test.append(name)\n",
    "        \n",
    "myset = set(test)\n",
    "print(myset)\n",
    "print(len(myset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_videos = list(myset)\n",
    "data_frames = []\n",
    "for video in test_videos:\n",
    "    tmp_df = df[df['info']==video] \n",
    "    data_frames.append(tmp_df)\n",
    "    \n",
    "test_df = pd.concat(data_frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total data:\n",
      "\n",
      "              features  video   info\n",
      "label                               \n",
      "adenoma          23319  23319  23319\n",
      "hiperplastic      8218   8218   8218\n",
      "serrated          7227   7227   7227\n",
      "\n",
      " ===== train data: ===== \n",
      "\n",
      "              features  video   info\n",
      "label                               \n",
      "adenoma          19158  19158  19158\n",
      "hiperplastic      6841   6841   6841\n",
      "serrated          6120   6120   6120\n",
      "amount of videos by polyp class: \n",
      "\n",
      "adenoma: 32\n",
      "hiperplastic: 17\n",
      "serrated: 12\n",
      "\n",
      " ===== test dataframe: ===== \n",
      "\n",
      "              features  video  info\n",
      "label                              \n",
      "adenoma           4161   4161  4161\n",
      "hiperplastic      1377   1377  1377\n",
      "serrated          1107   1107  1107\n",
      "amount of videos by polyp class: \n",
      "\n",
      "adenoma: 8\n",
      "hiperplastic: 4\n",
      "serrated: 3\n"
     ]
    }
   ],
   "source": [
    "train_df = df[~(df['info'].isin(test_df['info']))].reset_index(drop=True)\n",
    "\n",
    "print(\"total data:\\n\")\n",
    "print(df.groupby(['label']).count())\n",
    "\n",
    "print(\"\\n ===== train data: ===== \\n\")\n",
    "print(train_df.groupby(['label']).count())\n",
    "print(\"amount of videos by polyp class: \\n\")\n",
    "print(\"adenoma: {}\".format(len(train_df[train_df['label']=='adenoma']['video'].unique())))\n",
    "print(\"hiperplastic: {}\".format(len(train_df[train_df['label']=='hiperplastic']['video'].unique())))\n",
    "print(\"serrated: {}\".format(len(train_df[train_df['label']=='serrated']['video'].unique())))\n",
    "\n",
    "print(\"\\n ===== test dataframe: ===== \\n\")\n",
    "print(test_df.groupby(['label']).count())\n",
    "print(\"amount of videos by polyp class: \\n\")\n",
    "print(\"adenoma: {}\".format(len(test_df[test_df['label']=='adenoma']['video'].unique())))\n",
    "print(\"hiperplastic: {}\".format(len(test_df[test_df['label']=='hiperplastic']['video'].unique())))\n",
    "print(\"serrated: {}\".format(len(test_df[test_df['label']=='serrated']['video'].unique())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"amount of frames by each video training sample:\")\n",
    "a = pd.DataFrame(train_df.groupby(['video']).count()['features'])\n",
    "a = a.sort_values(by='features', axis=0, ascending=True)\n",
    "a = a.reset_index()\n",
    "a.head(n=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_drop = a.head(n=10)\n",
    "to_drop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Minimo de frames en todos los videos**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minimo1 = 9999\n",
    "path = '../../../../pregrado/data/RGB/NBI/'\n",
    "clases = os.listdir(path)\n",
    "for clase in clases:\n",
    "    videos_path = path + clase\n",
    "    videos = os.listdir(videos_path)\n",
    "    for video in videos:\n",
    "        imgs_path = videos_path + '/' + video\n",
    "        can = len(os.listdir(imgs_path))\n",
    "        if can<minimo1:\n",
    "            minimo1 = can\n",
    "            tipo = clase\n",
    "        \n",
    "print(\"la clase {} tiene menos imagenes {}\".format(tipo, minimo1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minimo2 = a.iloc[11]['features']\n",
    "print(\"the minimum of frames to be taking into account: \", minimo)\n",
    "to_drop2 = list(to_drop['video'].values)\n",
    "nums = [int(item.split('_')[-1]) for item in to_drop2]\n",
    "print(\"======================\")\n",
    "print(\"adenoma videos to drop: {}\".format(np.count_nonzero(np.array(nums)<41)))\n",
    "print(\"hyperplastic videos to drop: {}\".format(np.count_nonzero( (np.array(nums)>40)&(np.array(nums)<62)) ))\n",
    "print(\"serrated videos to drop: {}\".format(np.count_nonzero( (np.array(nums)>61)&(np.array(nums)<77)) ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dropping videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_df2 = train_df[~(train_df['video'].isin(to_drop['video']))].reset_index(drop=True)\n",
    "#train_df = train_df2\n",
    "#print(\"train lenght before number of videos balancing: {}\".format(len(train_df)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Getting the lowest video sample possible**\n",
    "* Se busca tanto la clase como la cantidad de videos para determinada clase en el set train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if downsampling == True:\n",
    "    import random\n",
    "\n",
    "    random.seed(42)\n",
    "\n",
    "    train_videos = np.unique(train_df['info'].values)\n",
    "\n",
    "    ade, hip, ser = 0, 0, 0\n",
    "    adenomas, hiperplastic, serrated = [], [], []\n",
    "    lower = -999\n",
    "    train_videos = np.unique(train_df['info'].values)\n",
    "    for val in train_videos:\n",
    "        clase = val.split('/')[0]\n",
    "        if clase == 'adenoma':\n",
    "            ade += 1\n",
    "            adenomas.append(val)\n",
    "        elif clase == 'hiperplastic':\n",
    "            hip += 1\n",
    "            hiperplastic.append(val)\n",
    "        else:\n",
    "            ser += 1\n",
    "            serrated.append(val)\n",
    "\n",
    "    if ade<lower:\n",
    "        lower = ade\n",
    "        clase = \"ade\"\n",
    "\n",
    "    elif hip<lower:\n",
    "        lower = hip\n",
    "        clase = \"hip\"\n",
    "    else:\n",
    "        lower = ser\n",
    "        clase = \"ser\"\n",
    "\n",
    "    print(\"la clase con menos videos es: {} con {} videos\".format(clase, lower))\n",
    "\n",
    "    ade_samples = random.sample(adenomas, lower)\n",
    "    ade_samples = np.array(ade_samples)\n",
    "    hip_samples = random.sample(hiperplastic, lower)\n",
    "    hip_samples = np.array(hip_samples)\n",
    "    ser_samples = random.sample(serrated, lower)\n",
    "    ser_samples = np.array(ser_samples)\n",
    "\n",
    "    train_videos_tmp = np.concatenate((ade_samples, hip_samples, ser_samples), axis = 0)\n",
    "    \n",
    "    data_frames = []\n",
    "    for val in train_videos_tmp:\n",
    "        tmp_df = train_df[train_df['info']==val] \n",
    "        data_frames.append(tmp_df)\n",
    "\n",
    "    train_df = pd.concat(data_frames)\n",
    "\n",
    "    train_df = train_df.reset_index(drop=True)  \n",
    "    print(\"train lenght after number of videos balancing: {}\".format(len(train_df)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Downsamplig original embedding representation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for adenoma\n",
    "new_ade_dat, new_ade_lab, new_ade_vid = [], [], []\n",
    "ade_idx, hyp_idx, ser_idx = [], [], []\n",
    "\n",
    "(unique, counts) = np.unique(ade_vid, return_counts=True)\n",
    "freq = np.asarray((unique, counts)).T\n",
    "\n",
    "#for i, info in enumerate(freq):\n",
    "#    if freq[i][0] in to_drop2:\n",
    "#        ade_idx.append(i)\n",
    "#    else:\n",
    "#        pass\n",
    "#freq = np.delete(freq, ade_idx, axis=0)\n",
    "\n",
    "can = 0\n",
    "samples = int(minimo1/2)\n",
    "\n",
    "for i in range(len(freq)):\n",
    "    idx = can + int(int(freq[i][1])/2)\n",
    "    dat = ade_dat[idx-samples:idx+samples]\n",
    "    can = can + int(freq[i][1]) \n",
    "    \n",
    "    label = 'adenoma'\n",
    "    lab = [label]*(minimo1)\n",
    "    video = \"video_\" + str(i+1)\n",
    "    vid = [video]*(minimo1)\n",
    "    \n",
    "    new_ade_dat.extend(dat)\n",
    "    new_ade_lab.extend(lab)\n",
    "    new_ade_vid.extend(vid)\n",
    "\n",
    "new_ade_dat = np.array(new_ade_dat)\n",
    "new_ade_lab = np.array(new_ade_lab)\n",
    "new_ade_vid = np.array(new_ade_vid)\n",
    "\n",
    "#for hyperplastic\n",
    "new_hyp_dat, new_hyp_lab, new_hyp_vid = [], [], []\n",
    "(unique, counts) = np.unique(hyp_vid, return_counts=True)\n",
    "freq = np.asarray((unique, counts)).T\n",
    "\n",
    "#for i, info in enumerate(freq):\n",
    "#    if freq[i][0] in to_drop2:\n",
    "#        hyp_idx.append(i)\n",
    "#    else:\n",
    "#        pass\n",
    "#freq = np.delete(freq, hyp_idx, axis=0)\n",
    "\n",
    "can = 0\n",
    "samples = int(minimo1/2)\n",
    "\n",
    "for i in range(len(freq)):\n",
    "    idx = can + int(int(freq[i][1])/2)\n",
    "    dat = hyp_dat[idx-samples:idx+samples]\n",
    "    can = can + int(freq[i][1]) \n",
    "    \n",
    "    label = 'hiperplastic'\n",
    "    lab = [label]*(minimo1)\n",
    "    video = \"video_\" + str(40+i+1)\n",
    "    vid = [video]*(minimo1)\n",
    "    \n",
    "    new_hyp_dat.extend(dat)\n",
    "    new_hyp_lab.extend(lab)\n",
    "    new_hyp_vid.extend(vid)\n",
    "\n",
    "new_hyp_dat = np.array(new_hyp_dat)\n",
    "new_hyp_lab = np.array(new_hyp_lab)\n",
    "new_hyp_vid = np.array(new_hyp_vid)\n",
    "    \n",
    "#for serrated\n",
    "#for adenoma\n",
    "new_ser_dat, new_ser_lab, new_ser_vid = [], [], []\n",
    "(unique, counts) = np.unique(ser_vid, return_counts=True)\n",
    "freq = np.asarray((unique, counts)).T\n",
    "\n",
    "#for i, info in enumerate(freq):\n",
    "#    if freq[i][0] in to_drop2:\n",
    "#        ser_idx.append(i)\n",
    "#    else:\n",
    "#        pass\n",
    "#freq = np.delete(freq, ser_idx, axis=0)\n",
    "\n",
    "can = 0\n",
    "samples = int(minimo1/2)\n",
    "\n",
    "for i in range(len(freq)):\n",
    "    idx = can + int(int(freq[i][1])/2)\n",
    "    dat = ser_dat[idx-samples:idx+samples]\n",
    "    can = can + int(freq[i][1]) \n",
    "    \n",
    "    label = 'serrated'\n",
    "    lab = [label]*(minimo1)\n",
    "    video = \"video_\" + str(61+i+1)\n",
    "    vid = [video]*(minimo1)\n",
    "    \n",
    "    new_ser_dat.extend(dat)\n",
    "    new_ser_lab.extend(lab)\n",
    "    new_ser_vid.extend(vid)\n",
    "\n",
    "new_ser_dat = np.array(new_ser_dat)\n",
    "new_ser_lab = np.array(new_ser_lab)\n",
    "new_ser_vid = np.array(new_ser_vid)\n",
    "    \n",
    "print(\"==== data info ====\")\n",
    "print(\"ade dim: {}, amount of labels: {}, videos: {}\".format(new_ade_dat.shape, new_ade_lab.shape, new_ade_vid.shape))\n",
    "print(\"hyp dim: {}, amount of labels: {}, videos: {}\".format(new_hyp_dat.shape, new_hyp_lab.shape, new_hyp_vid.shape))\n",
    "print(\"ser dim: {}, amount of labels: {}, videos: {}\".format(new_ser_dat.shape, new_ser_lab.shape, new_ser_vid.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Concatenating data embeddings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = np.concatenate((new_ade_dat, new_hyp_dat, new_ser_dat), axis=0)\n",
    "labels = np.concatenate((new_ade_lab, new_hyp_lab, new_ser_lab), axis=0)\n",
    "videos = np.concatenate((new_ade_vid, new_hyp_vid, new_ser_vid), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(unique, counts) = np.unique(videos, return_counts=True)\n",
    "freq = np.asarray((unique, counts)).T\n",
    "print(len(freq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.DataFrame({'features': list(features), 'label': labels, 'video': videos}, columns=['features', 'label', 'video'])\n",
    "df2['info'] = df2['label'] + '/' + df2['video']\n",
    "df2.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = df2[df2['info'].isin(train_df['info'])].reset_index(drop=True)\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Stratified 4 fold cross validation**\n",
    "* 4.1. Dataframe creation from embedding information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.2 CycleGan train filtering data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 4.2 Stratified Kfold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if downsampling==True:\n",
    "    skf = StratifiedKFold(n_splits=4, shuffle=True, random_state=42)\n",
    "else:\n",
    "    skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ade = [\"adenoma/video_\" + str(i) for i in range(1,41)]\n",
    "hyp = [\"hiperplastic/video_\" + str(i) for i in range(41,62)]\n",
    "ser = [\"serrated/video_\" + str(i) for i in range(62,77)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "general_videos = np.concatenate((ade, hyp, ser), axis=0)\n",
    "general_videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_videos2 = list(set(general_videos) - set(test_videos))\n",
    "print(len(train_videos2))\n",
    "train_videos = list(set(train_videos2) -  set(train_videos_tmp))  \n",
    "len(train_videos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_videos_tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_videos = list(set(general_videos) - set(test_videos))\n",
    "if downsampling==True:\n",
    "    test_videos2 = list(set(train_videos) -  set(train_videos_tmp))  \n",
    "    clases = [video.split('/')[0] for video in train_videos_tmp]\n",
    "    values, counts = np.unique(clases, return_counts=True)\n",
    "else:\n",
    "    clases = [video.split('/')[0] for video in train_videos]\n",
    "    values, counts = np.unique(clases, return_counts=True)\n",
    "\n",
    "\n",
    "if downsampling==True:\n",
    "    print(\"train videos: \")\n",
    "    print(train_videos_tmp)\n",
    "\n",
    "    print(\"values: {}\".format(values))\n",
    "    print(\"counts: {}\".format(counts))\n",
    "\n",
    "    print(\"letting the lowest amount of frames and videos we have:\")\n",
    "    print(\"===== is any test videos in train video?=====\")\n",
    "    test_videos = test_videos + test_videos2\n",
    "    result = any(elem in test_videos for elem in train_videos_tmp)\n",
    "    print(result)\n",
    "else:\n",
    "    print(\"train videos: \")\n",
    "    print(train_videos)\n",
    "    print(\"values: {}\".format(values))\n",
    "    print(\"counts: {}\".format(counts))\n",
    "    \n",
    "    print(\"lonly taking the lowest amount of frames we have:\")\n",
    "    print(\"===== is any test videos in train video?=====\")\n",
    "    result = any(elem in test_videos for elem in train_videos)\n",
    "    print(result)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ade_label, hyp_label, ser_label = [], [], []\n",
    "if downsampling==True:\n",
    "    train_videos = train_videos_tmp\n",
    "else:\n",
    "    pass\n",
    "   \n",
    "for video in train_videos:\n",
    "    clase = video.split('/')[0]\n",
    "    if clase == 'adenoma':\n",
    "        ade_label.append(0)\n",
    "    elif clase == 'hiperplastic':\n",
    "        hyp_label.append(1)\n",
    "    else:\n",
    "        ser_label.append(2)\n",
    "        \n",
    "labels = np.concatenate((ade_label, hyp_label, ser_label), axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>**Helper functions**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics(file_name, y_true, preds):\n",
    "    \n",
    "    print(\"*** metrics report for: \", file_name, \"***\")\n",
    "    txtfile.write(\"*** metrics report for: \" + file_name + \"***\\n\")\n",
    "    precision,recall,fscore,support=score(y_true, preds, average='weighted')\n",
    "    print ('Precision : {}'.format(precision))\n",
    "    print ('Recall    : {}'.format(recall))\n",
    "    print ('F-score   : {}'.format(fscore))\n",
    "    print ('Support   : {}'.format(support))\n",
    "    \n",
    "    txtfile.write('Precision :'+str(precision)+'\\n')\n",
    "    txtfile.write('Recall    :'+str(recall)+'\\n')\n",
    "    txtfile.write('F-score   :'+str(fscore)+'\\n')\n",
    "    txtfile.write('Support   :'+str(support)+'\\n')\n",
    "\n",
    "    return fscore    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_confusion_matrix(file_name, y_true, preds, fold):\n",
    "    from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "    \n",
    "    if downsampling==True:\n",
    "        save = '../confussionMatrix/' + generator + '/' + 'balanced_class' + file_name + str(fold) + '.png'\n",
    "    else:\n",
    "        save = '../confussionMatrix/' + generator + '/' + 'unbalanced_class' + file_name + str(fold) + '.png'\n",
    "        \n",
    "    target_names = ['adenoma', 'hiperplastic', 'serrated']\n",
    "    cm = confusion_matrix(y_true=y_test, y_pred=preds, normalize='true')\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=target_names)\n",
    "    disp = disp.plot(include_values=True, cmap=plt.cm.Blues, xticks_rotation='horizontal', values_format='.2f')\n",
    "    \n",
    "    plt.savefig(save)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ml_train(train_features, labels):\n",
    "        \n",
    "    print(\"Random forest 200 trees training...\")\n",
    "    forest = RandomForestClassifier(n_estimators=200, class_weight=\"balanced\", bootstrap=True, oob_score=True,\n",
    "                                    random_state=42)\n",
    "    forest.fit(train_features, labels)\n",
    "    \n",
    "    print(\"Support vector machine training...\")\n",
    "    svm_model = SVC(C=10, kernel='rbf', gamma='auto', class_weight='balanced', decision_function_shape='ovr')\n",
    "    svm_model.fit(train_features, labels)\n",
    "    \n",
    "    print(\"KNN 15 neighbors training...\")\n",
    "    knn = KNeighborsClassifier(n_neighbors=15, weights=\"distance\")\n",
    "    knn.fit(train_features, labels)\n",
    "    \n",
    "    return forest, svm_model, knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ml_test(test_features, labels, forest, svm_model, knn, fold):\n",
    "        \n",
    "    max_fscore = -9999\n",
    "    models = [\"RF200\", \"SVM\", \"KNN15\"]\n",
    "    for model in models:       \n",
    "        print(\"Testing with \", model)\n",
    "        if model == \"RF200\":\n",
    "            preds = forest.predict(test_features)\n",
    "            fscore = get_metrics(model, labels, preds)\n",
    "            if fscore > max_fscore:\n",
    "                max_fscore = fscore\n",
    "                experiment = model\n",
    "                preds2 = preds\n",
    "        elif model == \"SVM\":\n",
    "            preds = svm_model.predict(test_features)\n",
    "            fscore = get_metrics(model, labels, preds)\n",
    "            if fscore > max_fscore:\n",
    "                max_fscore = fscore\n",
    "                experiment = model\n",
    "                preds2 = preds\n",
    "        else:\n",
    "            preds = knn.predict(test_features)\n",
    "            fscore = get_metrics(model, labels, preds)\n",
    "            if fscore > max_fscore:\n",
    "                max_fscore = fscore\n",
    "                experiment = model\n",
    "                preds2 = preds\n",
    "            \n",
    "        \n",
    "    print(\"the \", experiment, \" experiment get max fscore: \", max_fscore)\n",
    "    \n",
    "    print(\"saving model...\")\n",
    "    if downsampling==True:\n",
    "        save_pth = '../models/embeddingClassification/checkPoints/' + generator+ '/' + 'balanced_class' \n",
    "    else:\n",
    "        save_pth = '../models/embeddingClassification/checkPoints/' + generator+ '/' + 'unbalanced_class'\n",
    "        \n",
    "    if experiment==\"RF200\":\n",
    "        file_name = save_pth + experiment + \"fold\" + str(fold) + '.joblib'\n",
    "        dump(forest, file_name)\n",
    "    elif experiment=='SVM':\n",
    "        file_name = save_pth + experiment + \"fold\" + str(fold) + '.joblib'\n",
    "        dump(svm_model, file_name) \n",
    "    else:\n",
    "        file_name = save_pth + experiment + \"fold\" + str(fold) + '.joblib'\n",
    "        dump(knn, file_name)\n",
    "    \n",
    "    print(\"CONFUSSION MATRIX:\")\n",
    "    make_confusion_matrix(experiment, labels, preds2, fold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Main**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txtfile = open('embClassificationMetricsBalancedClass' + generator + '.txt', 'w+')\n",
    "c = 0\n",
    "for train_index, test_index in skf.split(train_videos, labels):\n",
    "\n",
    "    x_train, x_test, y_train, y_test = [], [], [], []\n",
    "    x_train_fold, x_test_fold = [], []\n",
    "    c += 1\n",
    "    txtfile.write(\"***KFOLD ***\" +str(c))\n",
    "    print(\"***KFOLD ***\", c)\n",
    "    \n",
    "    for index in train_index:\n",
    "        fold = train_videos[index]\n",
    "        x_train_fold.append(fold)\n",
    "    for index in test_index:\n",
    "        fold = train_videos[index]\n",
    "        x_test_fold.append(fold)\n",
    "    \n",
    "    #tomo el nombre tanto de videos como labels por el indice\n",
    "    #x_train_fold, x_test_fold = train_videos[train_index], train_videos[test_index]\n",
    "    #y_train_fold, y_test_fold = labels[train_index], labels[test_index]\n",
    "    print(\"folds created!\")\n",
    "    #make train dataframe  \n",
    "    for i, data in enumerate(x_train_fold):\n",
    "        tmp_train_df= train_df[train_df['info']==data]\n",
    "        x_train.append(tmp_train_df)\n",
    "\n",
    "    train_df2 = pd.concat(x_train)\n",
    "    train_df2 = train_df2.reset_index(drop=True)\n",
    "    y_train = train_df2['label']\n",
    "    y_train = y_train.to_numpy()\n",
    "    \n",
    "    print(\"train info: \")\n",
    "    unique, counts = np.unique(y_train, return_counts=True)\n",
    "\n",
    "    print(np.asarray((unique, counts)).T)\n",
    "    \n",
    "    #make test dataframe\n",
    "    for i, data in enumerate(x_test_fold):\n",
    "        tmp_test_df= train_df[train_df['info']==data]\n",
    "        x_test.append(tmp_test_df)\n",
    "\n",
    "    test_df2 = pd.concat(x_test)\n",
    "    test_df2 = test_df2.reset_index(drop=True)\n",
    "    y_test = test_df2['label']\n",
    "    y_test = y_test.to_numpy()\n",
    "    \n",
    "    print(\"test info: \")\n",
    "    unique, counts = np.unique(y_test, return_counts=True)\n",
    "\n",
    "    print(np.asarray((unique, counts)).T)\n",
    "    \n",
    "    print(\"features management: \")\n",
    "    train_features = []\n",
    "    for i in range(len(train_df2)):\n",
    "        tmp_features = train_df2.loc[i]['features']\n",
    "        train_features.append(tmp_features)\n",
    "\n",
    "    train_features = np.array(train_features)\n",
    "    \n",
    "    test_features = []\n",
    "    for i in range(len(test_df2)):\n",
    "        tmp_features = test_df2.loc[i]['features']\n",
    "        test_features.append(tmp_features)\n",
    "\n",
    "    test_features = np.array(test_features) \n",
    "\n",
    "    print(\"ml models training...\")\n",
    "    \n",
    "    forest, svm_model, knn = ml_train(train_features, y_train)\n",
    "    \n",
    "\n",
    "    print(\"ml models testing...\")\n",
    "    ml_test(test_features, y_test, forest, svm_model, knn, c)\n",
    "\n",
    "txtfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='red'>Visual ML prediction</font>\n",
    "### Machine learning loading ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = '../models/embeddingClassification/checkPoints/'\n",
    "if generator == \"GenA\":\n",
    "    if downsampling==True:\n",
    "        classifier = load(root+'GenA/balanced_class/balanced_classKNN15fold4.joblib')\n",
    "    else:\n",
    "        classifier = load(root+'GenA/unbalanced_class/balanced_classKNN15fold4.joblib')\n",
    "    \n",
    "else:\n",
    "    if downsampling==True:\n",
    "        classifier = load(root+'GenB/balanced_class/balanced_classRF200fold4.joblib')\n",
    "    else:\n",
    "        classifier = load(root+'GenB/unbalanced_class/balanced_classKNN15fold4.joblib')\n",
    "\n",
    "print(\"classifier loaded! from generator: {} and downsampled: {}\".format(generator, downsampling))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering for cycleGan data test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = test_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifier predicting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features = []\n",
    "for i in range(len(test_df)):\n",
    "    tmp_features = test_df.loc[i]['features']\n",
    "    test_features.append(tmp_features)\n",
    "\n",
    "test_features = np.array(test_features)\n",
    "print(\"features shape: {}\".format(test_features.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = classifier.predict(test_features)\n",
    "print(predictions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = test_df['label'].values\n",
    "y_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.classification_report(y_true, predictions, target_names=[\"adenoma\",\"hiperplastico\", \"serrated\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['predicted'] = predictions\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA data projection and visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=3)\n",
    "pca_result = pca.fit_transform(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['pca-one'] = pca_result[:,0]\n",
    "test_df['pca-two'] = pca_result[:,1] \n",
    "test_df['pca-three'] = pca_result[:,2]\n",
    "\n",
    "print('Explained variation per principal component: {}'.format(pca.explained_variance_ratio_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(test_df, x=\"pca-one\", y=\"pca-two\", color='predicted', hover_name=\"label\", hover_data=[\"video\"],\n",
    "                 opacity=0.5)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
